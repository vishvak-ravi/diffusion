{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "torch.set_default_tensor_type(torch.cuda.FloatTensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 500, 781])\n",
      "torch.Size([1, 3, 500, 752])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torchvision.datasets import Flowers102\n",
    "from torchvision.transforms import Normalize, Compose, ToTensor\n",
    "\n",
    "dataset = Flowers102(\"data/\", transform=Compose([ToTensor(), Normalize(0.5, 0.5)]), download=True)\n",
    "dataloader = DataLoader(dataset, 1, shuffle=True, drop_last=True)\n",
    "\n",
    "count = 0\n",
    "for (X, y) in dataloader:\n",
    "    if count > 1:\n",
    "        break\n",
    "    print(X.shape)\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking conv dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameters\n",
    "n = 32  # Size of the square matrix\n",
    "in_channels = 1  # Number of input channels\n",
    "out_channels = 1  # Number of output channels\n",
    "kernel_size = 3  # Size of the convolutional kernel\n",
    "stride = 2  # Stride of the convolution\n",
    "padding = 1  # Padding of the convolution\n",
    "\n",
    "# Create a random input tensor\n",
    "input_tensor = torch.randn(1, in_channels, n, n)\n",
    "print(f\"Shape of the input tensor: {input_tensor.size()}\")\n",
    "\n",
    "# Create a 2D convolutional layer\n",
    "down_conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "\n",
    "# Pass the input tensor through the convolutional layer\n",
    "downed_tensor = down_conv(input_tensor)\n",
    "\n",
    "# Print the dimensions of the output tensor\n",
    "print(f\"Shape after down: {downed_tensor.size()}\")\n",
    "\n",
    "\n",
    "t_in_channels = in_channels  # Number of input channels\n",
    "t_out_channels = out_channels  # Number of output channels\n",
    "t_kernel_size = kernel_size + 1  # Size of the convolutional kernel\n",
    "t_stride = 2  # Stride of the convolution\n",
    "t_padding = 1  # Padding of the convolution\n",
    "\n",
    "up_conv = nn.ConvTranspose2d(t_in_channels, t_out_channels, t_kernel_size, t_stride, t_padding)\n",
    "upped_tensor = up_conv(downed_tensor)\n",
    "print(f\"Shape after up: {up_conv(downed_tensor).size()}\")\n",
    "\n",
    "print(f\"Shape matched: {input_tensor.size() == upped_tensor.size()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking MHA implementation using einsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# real impl\n",
    "n = 8\n",
    "c = 4\n",
    "h = 32\n",
    "w = h\n",
    "\n",
    "embed_dim=h*w\n",
    "num_heads=4\n",
    "\n",
    "X = torch.randn(n, c, h, w)\n",
    "print(X.shape)\n",
    "MHA = nn.MultiheadAttention(embed_dim=embed_dim, num_heads=num_heads)\n",
    "conv = nn.Conv2d(in_channels=c, out_channels=3 * c, kernel_size=3,padding=1)\n",
    "Q, K, V = torch.reshape(conv(X), (X.shape[0], X.shape[1], X.shape[2] * X.shape[3], 3)).unbind(3)\n",
    "Y = MHA(Q, K, V)[0]\n",
    "img_dim = int(np.sqrt(embed_dim))\n",
    "Y.reshape(X.shape[0], X.shape[1], img_dim, img_dim)\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## simple single-head scaled self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 30 # data points\n",
    "d = 32 # dim of data\n",
    "heads = 1\n",
    "D_k = d # dim of query keys\n",
    "\n",
    "X = torch.randn(n, d)\n",
    "Q = K = V = X\n",
    "attn = torch.softmax(Q.matmul(K.T) / D_k, 1)\n",
    "Y = attn.matmul(V)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## with learned params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 30 # data points\n",
    "d = 32 # dim of data\n",
    "heads = 1\n",
    "D_k = d # dim of query keys\n",
    "\n",
    "class SHSelfAttention(nn.Module):\n",
    "    def __init__(self, heads, key_dim):\n",
    "        self."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 4\n",
    "c = 3\n",
    "h = 4\n",
    "w = 4\n",
    "emb_dim = 5\n",
    "\n",
    "linear = nn.Linear(in_features=emb_dim, out_features=c)\n",
    "\n",
    "\n",
    "x = torch.randn(n, c, h, w)\n",
    "emb = torch.randn(n, emb_dim)\n",
    "\n",
    "(x + linear(emb).unsqueeze(-1).unsqueeze(-1)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "a_json =  json.dumps(\"/mnt/meg/vishravi/diffusion/src/configs/attn.json\")\n",
    "some_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffusion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
